{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simpleParadox/RE_656/blob/main/CMPUT_656_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcLYKVX89f9q",
        "outputId": "99f661ff-0849-4f64-865d-d2850ebb2969"
      },
      "id": "OcLYKVX89f9q",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "271868ee",
      "metadata": {
        "id": "271868ee"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "# from keras.callbacks import Callback\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "75d4aba2",
      "metadata": {
        "id": "75d4aba2"
      },
      "outputs": [],
      "source": [
        "import tokenization\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2a30a98e",
      "metadata": {
        "id": "2a30a98e",
        "outputId": "8e0097f0-85ac-4521-e24d-8e73f41120dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "87a850d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87a850d1",
        "outputId": "7eecdcd0-80de-49f5-8baf-5d26765de7a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n",
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "relations_path = '/content/drive/Shareddrives/CMPUT 656 Data and Results/Processed Data/Input_500_29_relation.tsv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1moM-Vq-Ig3",
        "outputId": "ae17130c-1be6-45c4-cd08-8c92bae5d2ef"
      },
      "id": "i1moM-Vq-Ig3",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a7644a8c",
      "metadata": {
        "id": "a7644a8c"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(relations_path, encoding='utf-8', sep = '\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f2ea234a",
      "metadata": {
        "id": "f2ea234a"
      },
      "outputs": [],
      "source": [
        "train_data.fillna(\"\", inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "88e6c222",
      "metadata": {
        "id": "88e6c222"
      },
      "outputs": [],
      "source": [
        "# Shuffle data so that there is a higher chance of the train and test data being from the same distribution.\n",
        "train_data = shuffle(train_data, random_state = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "de315f0a",
      "metadata": {
        "id": "de315f0a",
        "outputId": "b8c1d609-e2e3-4ce5-e0bb-e51ca50096d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(589617, 2)\n"
          ]
        }
      ],
      "source": [
        "# Now read the rows, convert them into strings and then only keep the unique ones.\n",
        "sentences_and_lables = np.array([[' '.join(map(str, row[:-1].tolist())).strip(), row[-1]] for row in train_data.iloc[:,:].values])\n",
        "print(sentences_and_lables.shape)\n",
        "sentences = sentences_and_lables[:, 0]\n",
        "#labels = sentences_and_lables[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a767941a",
      "metadata": {
        "id": "a767941a"
      },
      "outputs": [],
      "source": [
        "label = preprocessing.LabelEncoder()\n",
        "y = label.fit_transform(train_data['relation'])\n",
        "label_mappings = integer_mapping = {i: l for i, l in enumerate(label.classes_)}\n",
        "# y = to_categorical(y) # doing this later."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_mappings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK1sMsF4wKn2",
        "outputId": "4fd72ba9-723d-496d-fdd7-384e0a369cfe"
      },
      "id": "aK1sMsF4wKn2",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'None',\n",
              " 1: 'award.award_nominated_work.award_nominations..award.award_nomination.award_nominee',\n",
              " 2: 'book.author.works_written',\n",
              " 3: 'book.book.genre',\n",
              " 4: 'business.company.industry',\n",
              " 5: 'education.educational_institution.students_graduates..education.educational_institution.students_graduates',\n",
              " 6: 'film.actor.film..film.performance.character',\n",
              " 7: 'film.director.film',\n",
              " 8: 'film.film.country',\n",
              " 9: 'film.film.genre',\n",
              " 10: 'film.film.language',\n",
              " 11: 'film.film.music',\n",
              " 12: 'film.film.production_companies',\n",
              " 13: 'film.performance.actor..film.performance.film',\n",
              " 14: 'film.producer.film',\n",
              " 15: 'film.writer.film',\n",
              " 16: 'government.political_party.politicians_in_this_party',\n",
              " 17: 'location.location.contains',\n",
              " 18: 'music.artist.album',\n",
              " 19: 'music.artist.origin',\n",
              " 20: 'people.deceased_person.place_of_death',\n",
              " 21: 'people.person.nationality',\n",
              " 22: 'people.person.parents',\n",
              " 23: 'people.person.place_of_birth',\n",
              " 24: 'people.person.profession',\n",
              " 25: 'people.person.religion',\n",
              " 26: 'people.person.spouse_s..people.marriage.spouse',\n",
              " 27: 'soccer.football_position.players',\n",
              " 28: 'sports.sports_team.roster..sports.sports_team_roster.player'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba31cda9",
      "metadata": {
        "id": "ba31cda9",
        "outputId": "120657a7-0381-4dc3-be86-8845daaacb08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[27  9 21  8 12]\n"
          ]
        }
      ],
      "source": [
        "print(y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c51d433",
      "metadata": {
        "id": "0c51d433",
        "outputId": "0b0f096c-2e03-42a1-d011-b81e94aff01d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Scorers and assistants Position Name DF  Marko Lomić Marko Lomić'\n",
            " 'Short films Title Genre  Bramadero Bramadero  Erotic'\n",
            " 'First Team Squad Name Nationality  Dídac Vilà Dídac Vilà  Spain'\n",
            " '2010s Film Country  Mother Mother 2009 film   South Korea South Korea'\n",
            " '2000s Film Studio(s)  Monster House Monster House film   Relativity Media Relativity Media ImageMovers ImageMovers and Amblin Entertainment Amblin Entertainment']\n"
          ]
        }
      ],
      "source": [
        "print(sentences[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38feb66c",
      "metadata": {
        "id": "38feb66c"
      },
      "outputs": [],
      "source": [
        "m_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
        "bert_layer = hub.KerasLayer(m_url, trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4c035ac",
      "metadata": {
        "id": "f4c035ac"
      },
      "outputs": [],
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "        \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len-len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "        \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f7e990c",
      "metadata": {
        "id": "1f7e990c"
      },
      "outputs": [],
      "source": [
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    \n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    \n",
        "    print(tf.shape(sequence_output))\n",
        "    clf_output = sequence_output[:, :, :]\n",
        "    print(tf.shape(clf_output))\n",
        "    \n",
        "    lay = tf.keras.layers.Conv1D(filters=8, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\")(clf_output)\n",
        "    lay = tf.keras.layers.MaxPooling1D(2, 2)(lay)\n",
        "    #lay = tf.keras.layers.LSTM(2, return_sequences=True, dropout=0.2)(lay)\n",
        "    lay = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8, return_sequences=True, dropout=0.2))(lay)\n",
        "    lay = tf.keras.layers.Flatten()(lay)\n",
        "    out = tf.keras.layers.Dense(29, activation='softmax')(lay)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    #model.compile(tf.keras.optimizers.Adam(lr=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=2e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    #model.compile(tf.keras.optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8978e352",
      "metadata": {
        "id": "8978e352"
      },
      "source": [
        "### Obtaining Train, test splits.\n",
        "###### In the train splits, we will have a separate validation split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067426d5",
      "metadata": {
        "id": "067426d5"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"training_relations/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0c4d4e",
      "metadata": {
        "id": "3b0c4d4e"
      },
      "outputs": [],
      "source": [
        "def get_labels(y_pred):\n",
        "    y_pred_label = np.zeros((len(y_pred),1))\n",
        "    print(y_pred_label.shape)\n",
        "    for index in range(len(y_pred)):\n",
        "        y_pred_label[index] = np.argmax(y_pred[index])\n",
        "    return y_pred_label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Do not run the following cell if using checkpointed files."
      ],
      "metadata": {
        "id": "supNxpCZ_xQv"
      },
      "id": "supNxpCZ_xQv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1273f00a",
      "metadata": {
        "id": "1273f00a"
      },
      "outputs": [],
      "source": [
        "# with tf.device('/device:GPU:0'):\n",
        "#     splits = 5 # For five fold cross-validation.\n",
        "#     #seeds = [i for i in range(splits)]  # Fix the seed value for reproducibility.\n",
        "#     seeds = [0] \n",
        "    \n",
        "#     val_dict = {}\n",
        "#     test_dict = {}\n",
        "\n",
        "#     # First get random train-test splits. Doesn't include validation, which will be obtained from the train set.\n",
        "#     for seed in seeds:\n",
        "#         x_t, x_test, y_t, y_test = train_test_split(sentences, y, random_state=seed, test_size=0.2)   # Global training and test sets.\n",
        "\n",
        "#         # Now get validation sets from each training set.\n",
        "# #         kf = KFold(n_splits=5, shuffle=False) # Setting shuffle=False because shuffled dataset already before.\n",
        "# #         fold_count = 0\n",
        "\n",
        "# #         for train_index, val_index in kf.split(x_t):\n",
        "# #             #print(x_t.shape)\n",
        "# #             #print(y_t.shape)\n",
        "# #             x_train, x_val = x_t[train_index], x_t[val_index]   # Training and validation features.\n",
        "# #             y_train, y_val = y_t[train_index], y_t[val_index]   # Training and validation labels.\n",
        "\n",
        "# #             #encode train data\n",
        "# #             max_len = 80\n",
        "# #             train_input = bert_encode(x_train, tokenizer, max_len=max_len)\n",
        "# #             train_labels = y_train\n",
        "# #             x_val = bert_encode(x_val, tokenizer, max_len=max_len)\n",
        "\n",
        "\n",
        "# #             model = build_model(bert_layer, max_len=max_len)\n",
        "# #             model.summary()\n",
        "# #             #checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "# #             checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
        "# #             earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "# #             train_sh = model.fit(\n",
        "# #             train_input, train_labels,\n",
        "# #             #validation_split=0.2,\n",
        "# #             validation_data=(x_val, y_val),\n",
        "# #             epochs=2,\n",
        "# #             callbacks=[checkpoint, earlystopping],\n",
        "# #             batch_size=16,\n",
        "# #             verbose=1)\n",
        "\n",
        "# #             # Validation sets can be used for hyperparamter tuning.  \n",
        "# #             val_dict[str(seed) + str(fold_count)] = train_sh.history\n",
        "# #             fold_count += 1\n",
        "\n",
        "#         #encode whole train data\n",
        "#         max_len = 80\n",
        "#         train_input = bert_encode(x_t, tokenizer, max_len=max_len)\n",
        "#         train_labels = y_t\n",
        "        \n",
        "#         #train model on whole train data\n",
        "#         model = build_model(bert_layer, max_len=max_len)\n",
        "#         model.summary()\n",
        "#         #checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "#         checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
        "#         earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
        "#         train_sh = model.fit(\n",
        "#         train_input, train_labels,\n",
        "#         validation_split=0,\n",
        "#         epochs=6,\n",
        "#         callbacks=[checkpoint, earlystopping],\n",
        "#         batch_size=16,\n",
        "#         verbose=1)\n",
        "\n",
        "#         #encode test data \n",
        "#         test_input = bert_encode(x_test, tokenizer, max_len=max_len)\n",
        "\n",
        "#         # Evaluate the model on the test data using `evaluate`\n",
        "#         print(\"Evaluate on test data for \", seed)\n",
        "#         results = model.evaluate(test_input, y_test, batch_size=16)\n",
        "\n",
        "#         #calculate F1-score\n",
        "#         y_pred = model.predict(test_input, verbose=1)\n",
        "#         y_pred_label = get_labels(y_pred)\n",
        "#         f1_value = f1_score(y_test, y_pred_label, average='macro')\n",
        "#         results.append(f1_value)\n",
        "#         print(\"test loss, test acc, F1-score:\", results)\n",
        "\n",
        "#         test_dict[seed] = results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbcc0f85",
      "metadata": {
        "id": "fbcc0f85"
      },
      "outputs": [],
      "source": [
        "# with open('results seed=0 batch_size=16.txt','w') as data: \n",
        "#     data.write(str(val_dict))\n",
        "#     data.write('\\n')\n",
        "#     data.write(str(test_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The following section is for loading the checkpoint weights and then obtaining the confusion matrix."
      ],
      "metadata": {
        "id": "FeWlf9nu_Pcj"
      },
      "id": "FeWlf9nu_Pcj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d262a9e",
      "metadata": {
        "id": "3d262a9e",
        "outputId": "ec6b3f57-b86e-4985-ebc8-9dcacb518539",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 80, 768], name='tf.compat.v1.shape/Shape:0', description=\"created by layer 'tf.compat.v1.shape'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, 80, 768], name='tf.compat.v1.shape_1/Shape:0', description=\"created by layer 'tf.compat.v1.shape_1'\")\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_word_ids (InputLayer)    [(None, 80)]         0           []                               \n",
            "                                                                                                  \n",
            " input_mask (InputLayer)        [(None, 80)]         0           []                               \n",
            "                                                                                                  \n",
            " segment_ids (InputLayer)       [(None, 80)]         0           []                               \n",
            "                                                                                                  \n",
            " keras_layer (KerasLayer)       [(None, 768),        109482241   ['input_word_ids[0][0]',         \n",
            "                                 (None, 80, 768)]                 'input_mask[0][0]',             \n",
            "                                                                  'segment_ids[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 80, 768)     0           ['keras_layer[0][1]']            \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 80, 8)        30728       ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 40, 8)        0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 40, 16)       1088        ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 640)          0           ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 29)           18589       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,532,646\n",
            "Trainable params: 50,405\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate on test data for  0\n",
            "1302/7371 [====>.........................] - ETA: 19:23"
          ]
        }
      ],
      "source": [
        "#code for loading weights from checkpoint\n",
        "with tf.device('/device:GPU:0'):\n",
        "    results = []\n",
        "    splits = 5 # For five fold cross-validation.\n",
        "    #seeds = [i for i in range(splits)]  # Fix the seed value for reproducibility.\n",
        "    seeds = [0, 1, 2, 3, 4]\n",
        "    \n",
        "    val_dict = {}\n",
        "    test_dict = {}\n",
        "    cms = []\n",
        "    # First get random train-test splits. Doesn't include validation, which will be obtained from the train set.\n",
        "    for seed in seeds:\n",
        "        x_t, x_test, y_t, y_test = train_test_split(sentences, y, random_state=seed, test_size=0.2)   # Global training and test sets.\n",
        "\n",
        "        # Evaluate the new model\n",
        "        max_len = 80\n",
        "        new_model = build_model(bert_layer, max_len=max_len)\n",
        "        new_model.summary()\n",
        "        # Loads the weights for new model\n",
        "        checkpoint_path = f\"/content/drive/Shareddrives/CMPUT 656 Data and Results/Result 29 Relations/Seed {seed}/training_relations/cp.ckpt\"\n",
        "        training_relations_path = f\"/content/drive/Shareddrives/CMPUT 656 Data and Results/Result 29 Relations/Seed {seed}/training_relations/\"\n",
        "        # /content/drive/Shareddrives/CMPUT 656 Data and Results/Result 29 Relations/Seed 0/training_relations/cp.ckpt.data-00000-of-00001\n",
        "        \n",
        "\n",
        "        new_model.load_weights(checkpoint_path)\n",
        "\n",
        "        #encode test data\n",
        "        test_input = bert_encode(x_test, tokenizer, max_len=max_len)\n",
        "\n",
        "        # Evaluate the model on the test data using `evaluate`\n",
        "        print(\"Evaluate on test data for \", seed)\n",
        "        # results = new_model.evaluate(test_input, y_test, batch_size=16)\n",
        "\n",
        "        #calculate F1-score\n",
        "        y_pred = new_model.predict(test_input, verbose=1)\n",
        "        y_pred_label = get_labels(y_pred)\n",
        "        f1_value = f1_score(y_test, y_pred_label, average='macro')\n",
        "        np.savez_compressed(training_relations_path +  \"predictions.npz\", y_pred_label)\n",
        "        cm = confusion_matrix(y_pred_label, y_test, labels=label_mappings.keys())\n",
        "        cms.append(cm)\n",
        "\n",
        "        results.append(f1_value)\n",
        "        print(\"Test loss, Test acc, F1-score:\", results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "averaged_cms = np.mean(cms, axis=0)\n",
        "cms_df = pd.DataFrame(averaged_cms, index = [value for value in label_mappings.values()],\n",
        "                      columns=[value for value in label_mappings.values()])\n",
        "sns.heatmap(cms_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jUlmE6gXzCnX"
      },
      "id": "jUlmE6gXzCnX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of CMPUT 656 Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}